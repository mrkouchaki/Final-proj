{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a9cb9c7",
   "metadata": {},
   "source": [
    "## My GraphLLM Model for MultiModal graph-text interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "import dgl\n",
    "print(\"DGL version:\", dgl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "import torch.nn as nn\n",
    "from dgl.data.utils import save_graphs, load_graphs\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "# I set this to save processed data and preventing reprocessing for mutiple run\n",
    "SAVE_DIR = 'processed_data'\n",
    "FORCE_NEW_RUN = False  # If True start a new run and overwrite\n",
    "\n",
    "if FORCE_NEW_RUN or not os.path.exists(os.path.join(SAVE_DIR, \"latest_run\")):\n",
    "    print(\"Starting a new run...\")\n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.makedirs(SAVE_DIR)\n",
    "    CURRENT_RUN_DIR = os.path.join(SAVE_DIR, \"latest_run\")\n",
    "    os.makedirs(CURRENT_RUN_DIR, exist_ok=True)\n",
    "else:\n",
    "    print(\"Reusing existing saved data...\")\n",
    "    CURRENT_RUN_DIR = os.path.join(SAVE_DIR, \"latest_run\")\n",
    "\n",
    "\n",
    "# Initialize our Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "\n",
    "# Since Data is Large I put this valve to limit it sometimes\n",
    "NODE_LIMIT = None\n",
    "EDGE_LIMIT = None\n",
    "\n",
    "DATASET_PATH = '/home/reza/ML PJ/Model1/implementation/my_twiBot20_1'\n",
    "\n",
    "# Load node data\n",
    "with open(os.path.join(DATASET_PATH, 'node.json'), 'r') as f:\n",
    "    nodes = json.load(f)\n",
    "\n",
    "\n",
    "print(f\"Number of nodes processed: {len(nodes)}\")\n",
    "if NODE_LIMIT:\n",
    "    nodes = nodes[:NODE_LIMIT]\n",
    "\n",
    "# Show some samples of nodes\n",
    "print(\"First 3 entries in node.json:\")\n",
    "for node in nodes[:3]:\n",
    "    print(json.dumps(node, indent=2))\n",
    "    print('-' * 40)\n",
    "user_count, tweet_count = 0, 0\n",
    "\n",
    "for node in nodes:\n",
    "    node_id = node['id']\n",
    "    if node_id.startswith('u') and user_count < 3:\n",
    "        print(\"User node:\")\n",
    "        print(json.dumps(node, indent=2))\n",
    "        print('-' * 40)\n",
    "        user_count += 1\n",
    "    elif node_id.startswith('t') and tweet_count < 3:\n",
    "        print(\"Tweet node:\")\n",
    "        print(json.dumps(node, indent=2))\n",
    "        print('-' * 40)\n",
    "        tweet_count += 1\n",
    "    \n",
    "    if user_count >= 3 and tweet_count >= 3:\n",
    "        break\n",
    "\n",
    "# edge data\n",
    "edges_df = pd.read_csv(os.path.join(DATASET_PATH, 'edge.csv'))\n",
    "print(\"\\nFirst 5 entries in edge.csv:\")\n",
    "print(edges_df.head())\n",
    "\n",
    "# label nodes both users and tweets and filter unlabeled data in our dataset\n",
    "label_mapping = {'bot': 1, 'human': 0}\n",
    "labels_df = pd.read_csv(os.path.join(DATASET_PATH, 'label.csv'))\n",
    "print(\"\\nFirst 5 entries in label.csv:\")\n",
    "print(labels_df.head())\n",
    "\n",
    "labels_dict = {row['id']: label_mapping[row['label']] for _, row in labels_df.iterrows()}\n",
    "labeled_user_ids = set(labels_dict.keys())\n",
    "print(f\"Number of labeled users: {len(labeled_user_ids)}\")\n",
    "\n",
    "node_types = {'user': [], 'tweet': []}\n",
    "edge_types = {'friend': [], 'follow': [], 'post': []}\n",
    "\n",
    "for node in nodes:\n",
    "    node_id = node['id']\n",
    "    if node_id.startswith('u') and node_id in labeled_user_ids:\n",
    "        node_types['user'].append(node_id)\n",
    "    elif node_id.startswith('t'):\n",
    "        node_types['tweet'].append(node_id)\n",
    "\n",
    "print(f\"Filtered number of user nodes with labels: {len(node_types['user'])}\")\n",
    "print(f\"Total tweet nodes: {len(node_types['tweet'])}\")\n",
    "\n",
    "node_id_to_index = {node_id: idx for idx, node_id in enumerate(node_types['user'] + node_types['tweet'])}\n",
    "\n",
    "filtered_edges_df = edges_df[\n",
    "    edges_df['source_id'].isin(labeled_user_ids) &\n",
    "    edges_df['target_id'].isin(node_id_to_index)\n",
    "]\n",
    "print(f\"Number of edges after filtering: {len(filtered_edges_df)}\")\n",
    "\n",
    "print(\"Mapping source and target IDs to indices...\")\n",
    "filtered_edges_df['src_index'] = filtered_edges_df['source_id'].map(node_id_to_index)\n",
    "filtered_edges_df['dst_index'] = filtered_edges_df['target_id'].map(node_id_to_index)\n",
    "print(\"Mapping complete.\")\n",
    "\n",
    "# Now process edges\n",
    "filtered_edges = {'friend': [], 'follow': [], 'post': []}\n",
    "\n",
    "for rel in filtered_edges:\n",
    "    edge_type_df = filtered_edges_df[filtered_edges_df['relation'] == rel]\n",
    "    filtered_edges[rel] = list(zip(edge_type_df['src_index'], edge_type_df['dst_index']))\n",
    "    print(f\"Processed {rel} edges: {len(filtered_edges[rel])} added.\")\n",
    "\n",
    "# Show counts of each edge type\n",
    "print(\"\\nFiltered Edge type counts:\")\n",
    "for edge_type, edges in filtered_edges.items():\n",
    "    print(f\"{edge_type}: {len(edges)} edges\")\n",
    "\n",
    "print(\"Edge separation by type completed.\")\n",
    "\n",
    "if any(len(edges) > 0 for edges in filtered_edges.values()):\n",
    "    data_dict = {\n",
    "        ('user', 'friend', 'user'): (torch.tensor([e[0] for e in filtered_edges['friend']]),\n",
    "                                     torch.tensor([e[1] for e in filtered_edges['friend']])),\n",
    "        ('user', 'follow', 'user'): (torch.tensor([e[0] for e in filtered_edges['follow']]),\n",
    "                                     torch.tensor([e[1] for e in filtered_edges['follow']])),\n",
    "        ('user', 'post', 'tweet'): (torch.tensor([e[0] for e in filtered_edges['post']]),\n",
    "                                    torch.tensor([e[1] for e in filtered_edges['post']]))\n",
    "    }\n",
    "    print(\"Filtered edge tensors created.\")\n",
    "else:\n",
    "    print(\"No edges were found. Check the filtering criteria and data consistency.\")\n",
    "\n",
    "\n",
    "print(\"Strat counting connected tweet ids\")\n",
    "\n",
    "filtered_post_edges = edges_df[(edges_df['relation'] == 'post') & (edges_df['source_id'].isin(labeled_user_ids))]\n",
    "connected_tweet_ids = set(filtered_post_edges['target_id'])\n",
    "\n",
    "#checkpoints\n",
    "print(f\"Number of connected tweet IDs: {len(connected_tweet_ids)}\")\n",
    "print(\"Sample connected tweet IDs:\", list(connected_tweet_ids)[:2])\n",
    "connected_tweet_ids_in_index = connected_tweet_ids.intersection(node_id_to_index.keys())\n",
    "print(f\"Number of connected tweet IDs actually in node_id_to_index: {len(connected_tweet_ids_in_index)}\")\n",
    "\n",
    "\n",
    "# Step 2: build graph\n",
    "if any(len(edges) > 0 for edges in filtered_edges.values()):\n",
    "    print(\"\\nBuilding the DGL heterogeneous graph with filtered data...\")\n",
    "    G = dgl.heterograph(data_dict)\n",
    "    print(\"Filtered DGL heterogeneous graph created.\")\n",
    "    \n",
    "    for ntype in G.ntypes:\n",
    "        filtered_ids = [node_id_to_index[node_id] for node_id in node_types[ntype] if node_id in node_id_to_index]\n",
    "        if len(filtered_ids) == G.num_nodes(ntype):\n",
    "            G.nodes[ntype].data[dgl.NID] = torch.tensor(filtered_ids, dtype=torch.int64)\n",
    "        else:\n",
    "            print(f\"Warning: Mismatch in number of nodes for type '{ntype}'. Expected {G.num_nodes(ntype)}, got {len(filtered_ids)}.\")\n",
    "\n",
    "    print(\"\\n=== Graph Data Inspection ===\")\n",
    "    for ntype in G.ntypes:\n",
    "        print(f\"Number of nodes for type '{ntype}':\", G.num_nodes(ntype))\n",
    "        print(f\"Node data for '{ntype}':\", G.nodes[ntype].data.keys())\n",
    "        \n",
    "    \n",
    "    G.ndata[dgl.NTYPE] = {'user': torch.full((G.num_nodes('user'),), G.get_ntype_id('user'), dtype=torch.int32),'tweet': torch.full((G.num_nodes('tweet'),), G.get_ntype_id('tweet'), dtype=torch.int32)}\n",
    "\n",
    "    \n",
    "    # Step 3: Use LLM Tokenizer to embedd text data and generate features for user nodes\n",
    "    print(\"\\nGenerating BERT embeddings for user nodes...\")\n",
    "    user_features = []\n",
    "    for node in nodes:\n",
    "        node_id = node['id']\n",
    "        if node_id.startswith('u') and node_id in labeled_user_ids:\n",
    "            description = node.get('description', '')\n",
    "            inputs = tokenizer(description, return_tensors='pt', truncation=True, max_length=128)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = bert_model(**inputs)\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1).squeeze(0)  # Mean pooling for fixed-size embedding\n",
    "            \n",
    "            user_features.append(embedding)\n",
    "    \n",
    "    user_features_path = os.path.join(CURRENT_RUN_DIR, \"user_features.pt\")\n",
    "\n",
    "    if os.path.exists(user_features_path):\n",
    "        print(\"Loading precomputed user features...\")\n",
    "        G.nodes['user'].data['feat'] = torch.load(user_features_path)\n",
    "    else:\n",
    "        print(\"Computing user features...\")\n",
    "        G.nodes['user'].data['feat'] = torch.stack(user_features)\n",
    "        print(\"BERT embeddings assigned to user nodes.\")\n",
    "\n",
    "        torch.save(G.nodes['user'].data['feat'], user_features_path)\n",
    "        print(\"User features saved.\")\n",
    "\n",
    "\n",
    "    print(\"\\nGenerating embeddings for connected tweet nodes present in node_id_to_index...\")\n",
    "\n",
    "    # Prepare batches of tweet texts\n",
    "    tweet_texts = []\n",
    "    for node in nodes:\n",
    "        node_id = node['id']\n",
    "        if node_id in connected_tweet_ids_in_index and node_id.startswith('t'):\n",
    "            text = node.get('text', '')\n",
    "            tweet_texts.append((node_id, text))\n",
    "\n",
    "    batch_size = 20  # I set it based on my available memory\n",
    "    tweet_embeddings = {}\n",
    "    for i in tqdm(range(0, len(tweet_texts), batch_size), desc=\"Processing tweet batches\"):\n",
    "        batch = tweet_texts[i:i + batch_size]\n",
    "        texts = [text for _, text in batch]\n",
    "\n",
    "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "\n",
    "        for (tweet_id, _), embedding in zip(batch, embeddings):\n",
    "            tweet_embeddings[tweet_id] = embedding\n",
    "    \n",
    "    num_tweet_nodes = G.num_nodes('tweet')\n",
    "    embedding_dim = next(iter(tweet_embeddings.values())).shape[0]\n",
    "    print(f'Embedding dimension: {embedding_dim}')\n",
    "\n",
    "    # We need a tensor to hold features\n",
    "    tweet_features = torch.zeros((num_tweet_nodes, embedding_dim))\n",
    "    print(f'Initialized tweet_features tensor with shape: {tweet_features.shape}')\n",
    "    placeholder_embedding = torch.zeros(embedding_dim)\n",
    "\n",
    "    tweet_node_ids = [node_id for node_id in node_id_to_index if node_id.startswith('t')]\n",
    "    \n",
    "    # change the batch size for next process\n",
    "    batch_size = 1000 \n",
    "    # Process batches and assign embeddings directly to the preallocated tensor\n",
    "    for start in range(0, num_tweet_nodes, batch_size):\n",
    "        end = min(start + batch_size, num_tweet_nodes)\n",
    "        batch_ids = tweet_node_ids[start:end]\n",
    "        tweet_feature_batch = torch.stack([\n",
    "            tweet_embeddings.get(node_id, placeholder_embedding) for node_id in batch_ids\n",
    "        ])\n",
    "\n",
    "        print(f'\\nProcessing batch from index {start} to {end}')\n",
    "        print(f'Number of nodes in this batch: {len(batch_ids)}')\n",
    "        print(f'Tweet feature batch shape: {tweet_feature_batch.shape}')\n",
    "\n",
    "        tweet_features[start:end, :] = tweet_feature_batch\n",
    "\n",
    "    print(f'\\nFinal tweet_features tensor shape: {tweet_features.shape}')\n",
    "    \n",
    "    tweet_features_path = os.path.join(CURRENT_RUN_DIR, \"tweet_features.pt\")\n",
    "\n",
    "    if os.path.exists(tweet_features_path):\n",
    "        print(\"Loading precomputed tweet features...\")\n",
    "        G.nodes['tweet'].data['feat'] = torch.load(tweet_features_path)\n",
    "    else:\n",
    "        print(\"Computing tweet features...\")\n",
    "        G.nodes['tweet'].data['feat'] = tweet_features\n",
    "        print(\"BERT embeddings assigned to connected tweet nodes in the graph.\")\n",
    "\n",
    "        torch.save(G.nodes['tweet'].data['feat'], tweet_features_path)\n",
    "        print(\"Tweet features saved.\")\n",
    "\n",
    "\n",
    "    for ntype in G.ntypes:\n",
    "        G.nodes[ntype].data[dgl.NID] = torch.arange(G.num_nodes(ntype))\n",
    "\n",
    "    print(\"\\n=== Graph Data Inspection ===\")\n",
    "    print(\"Graph G node data:\", G.ndata.keys())\n",
    "    \n",
    "    graph_data_path = os.path.join(CURRENT_RUN_DIR, \"graph_data.bin\")\n",
    "\n",
    "    if os.path.exists(graph_data_path):\n",
    "        print(\"Loading precomputed graph...\")\n",
    "        G, _ = load_graphs(graph_data_path)\n",
    "        G = G[0]\n",
    "    else:\n",
    "        print(\"Building graph...\")\n",
    "        save_graphs(graph_data_path, [G])\n",
    "        print(\"Graph saved.\")\n",
    "\n",
    "    \n",
    "    for ntype in G.ntypes:\n",
    "        print(f\"Number of nodes for type '{ntype}':\", G.num_nodes(ntype))\n",
    "        print(f\"Node data for '{ntype}':\", G.nodes[ntype].data.keys())\n",
    "\n",
    "    print(\"\\n=== Edge Types and Counts ===\")\n",
    "    for etype in G.etypes:\n",
    "        print(f\"Edge type '{etype}' has {G.num_edges(etype)} edges\")\n",
    "        print(f\"Edge data for '{etype}':\", G.edges[etype].data.keys())\n",
    "  \n",
    "   \n",
    "else:\n",
    "    print(\"Graph creation aborted due to no available edges.\")\n",
    "\n",
    "\n",
    "texts, labels = [], []\n",
    "print(\"\\nPreparing texts and labels for labeled users...\")\n",
    "for node in nodes:\n",
    "    node_id = node['id']\n",
    "    if node_id.startswith('u') and node_id in labeled_user_ids:\n",
    "        label = labels_dict[node_id]\n",
    "        description = node.get('description', '')\n",
    "        tweets = ' '.join(node.get('tweet', []))\n",
    "        texts.append(description + ' ' + tweets)\n",
    "        labels.append(label)\n",
    "\n",
    "labels = torch.tensor(labels)\n",
    "print(\"Labels tensor created with shape:\", labels.shape)\n",
    "print(\"Sample labels:\", labels[:5].tolist())\n",
    "\n",
    "# Dump texts & Labels for next uses, this will help for repeated implementation\n",
    "\n",
    "texts_path = os.path.join(CURRENT_RUN_DIR, \"texts.pkl\")\n",
    "labels_path = os.path.join(CURRENT_RUN_DIR, \"labels.pt\")\n",
    "\n",
    "if os.path.exists(texts_path) and os.path.exists(labels_path):\n",
    "    print(\"Loading precomputed texts and labels...\")\n",
    "    with open(texts_path, \"rb\") as f:\n",
    "        texts = pickle.load(f)\n",
    "    labels = torch.load(labels_path)\n",
    "else:\n",
    "    print(\"Computing texts and labels...\")\n",
    "    with open(texts_path, \"wb\") as f:\n",
    "        pickle.dump(texts, f)\n",
    "    torch.save(labels, labels_path)\n",
    "    print(\"Texts and labels saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c36fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Building Graph, Learning phase starts\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from transformers import BertModel\n",
    "\n",
    "# ================================\n",
    "# Step 1: Graph Transformer Module\n",
    "# ================================\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_layers, edge_types, node_types):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.edge_types = edge_types\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.projection = nn.ModuleDict({\n",
    "            ntype: nn.Linear(hidden_dim, hidden_dim) for ntype in node_types\n",
    "        })\n",
    "\n",
    "    def forward(self, G):\n",
    "        h = {ntype: G.nodes[ntype].data['feat'] for ntype in G.ntypes}\n",
    "\n",
    "        for layer in self.layers:\n",
    "            updated_h = {ntype: [] for ntype in G.ntypes}\n",
    "\n",
    "            for etype in self.edge_types:\n",
    "                subgraph = G.edge_type_subgraph([etype])\n",
    "\n",
    "                src_type, _, dst_type = subgraph.to_canonical_etype(etype)\n",
    "\n",
    "                src_feat = subgraph.srcdata['feat']\n",
    "                dst_feat = subgraph.dstdata['feat']\n",
    "\n",
    "                # Apply attention mechanism\n",
    "                attn_output, _ = layer(src_feat.unsqueeze(1), dst_feat.unsqueeze(1), dst_feat.unsqueeze(1))\n",
    "                attn_output = attn_output.squeeze(1)\n",
    "                attn_output = self.layer_norm(attn_output)\n",
    "\n",
    "                updated_h[dst_type].append(attn_output)\n",
    "\n",
    "            for ntype in updated_h:\n",
    "                if updated_h[ntype]:\n",
    "                    aggregated_features = torch.stack(updated_h[ntype], dim=0).mean(dim=0)\n",
    "                    h[ntype] = self.projection[ntype](aggregated_features)\n",
    "\n",
    "        # mean pooling\n",
    "        final_graph_embedding = torch.cat([h[ntype] for ntype in h], dim=0).mean(dim=0, keepdim=True)\n",
    "        return final_graph_embedding\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Step 2: Text Transformer Module\n",
    "# ============================\n",
    "class TextTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextTransformer, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.hidden_dim = self.bert.config.hidden_size\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():  # Freeze LLM model for efficiency\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# ============================\n",
    "# Step 3: Fusion Layer\n",
    "# ============================\n",
    "class FusionLayer(nn.Module):\n",
    "    def __init__(self, graph_dim, text_dim, fusion_dim):\n",
    "        super(FusionLayer, self).__init__()\n",
    "        self.fc = nn.Linear(graph_dim + text_dim, fusion_dim)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=fusion_dim, num_heads=8)\n",
    "        self.layer_norm = nn.LayerNorm(fusion_dim)\n",
    "\n",
    "    def forward(self, graph_emb, text_emb):\n",
    "        graph_emb = graph_emb.repeat(text_emb.size(0), 1)\n",
    "        combined = torch.cat((graph_emb, text_emb), dim=-1)\n",
    "        combined = self.fc(combined)\n",
    "        #cross_attention\n",
    "        fused_emb, _ = self.cross_attention(combined.unsqueeze(1), combined.unsqueeze(1), combined.unsqueeze(1))\n",
    "        return self.layer_norm(fused_emb.squeeze(1))\n",
    "\n",
    "# ============================\n",
    "# Step 4: Classification head\n",
    "# ============================\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# ============================\n",
    "# Step 5: Build Multimodal Transformer\n",
    "# ============================\n",
    "\n",
    "class MultimodalTransformer(nn.Module):\n",
    "    def __init__(self, graph_dim, text_dim, fusion_dim, num_classes, edge_types, node_types):\n",
    "        super(MultimodalTransformer, self).__init__()\n",
    "        self.graph_transformer = GraphTransformer(\n",
    "            hidden_dim=graph_dim,\n",
    "            num_heads=8,\n",
    "            num_layers=2,\n",
    "            edge_types=edge_types,\n",
    "            node_types=node_types \n",
    "        )\n",
    "\n",
    "        self.text_transformer = TextTransformer()\n",
    "        self.fusion_layer = FusionLayer(graph_dim, text_dim, fusion_dim)\n",
    "        self.classifier = ClassificationHead(fusion_dim, num_classes)\n",
    "\n",
    "    def forward(self, G, input_ids, attention_mask):\n",
    "        # Process graph embeddings\n",
    "        graph_emb = self.graph_transformer(G)\n",
    "\n",
    "        # Process text embeddings\n",
    "        text_emb = self.text_transformer(input_ids, attention_mask)\n",
    "\n",
    "        # Fuse graph and text embeddings\n",
    "        fused_emb = self.fusion_layer(graph_emb, text_emb)\n",
    "\n",
    "        # Classification logits\n",
    "        logits = self.classifier(fused_emb)\n",
    "        return logits\n",
    "\n",
    "# ============================\n",
    "# Data\n",
    "# ============================    \n",
    "encoded_inputs = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    max_length=128,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "input_ids = encoded_inputs['input_ids']\n",
    "attention_mask = encoded_inputs['attention_mask']\n",
    "\n",
    "# Extract graph features\n",
    "graph_feats = G.ndata['feat']\n",
    "\n",
    "def preprocess_graph_features(graph_feats):\n",
    "    all_features = []\n",
    "    offsets = {}\n",
    "    current_offset = 0\n",
    "    for ntype, features in graph_feats.items():\n",
    "        offsets[ntype] = current_offset\n",
    "        all_features.append(features)\n",
    "        current_offset += features.size(0)\n",
    "    \n",
    "    combined_features = torch.cat(all_features, dim=0)\n",
    "    return combined_features, offsets\n",
    "\n",
    "# Preprocess graph features\n",
    "graph_feats_tensor, node_type_offsets = preprocess_graph_features(graph_feats)\n",
    "\n",
    "labels_tensor = labels.clone().detach()\n",
    "\n",
    "\n",
    "class MultimodalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, graph_feats_tensor, input_ids, attention_mask, labels):\n",
    "        self.graph_feats_tensor = graph_feats_tensor\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.graph_feats_tensor,\n",
    "            self.input_ids[idx],\n",
    "            self.attention_mask[idx],\n",
    "            self.labels[idx],\n",
    "        )\n",
    "\n",
    "# dataset creation\n",
    "dataset = MultimodalDataset(graph_feats_tensor, input_ids, attention_mask, labels_tensor)\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=8)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
    "  \n",
    "# ============================\n",
    "# Training Setup\n",
    "# ============================\n",
    "# Define hyperparameters and initialize the model\n",
    "graph_dim = 768 \n",
    "text_dim = 768\n",
    "fusion_dim = 512\n",
    "num_classes = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultimodalTransformer(\n",
    "    graph_dim=graph_dim,\n",
    "    text_dim=text_dim,\n",
    "    fusion_dim=fusion_dim,\n",
    "    num_classes=num_classes,\n",
    "    edge_types=G.etypes,\n",
    "    node_types=G.ntypes\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ============================\n",
    "# Training Pipeline\n",
    "# ============================\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize lists for metrics tracking\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "train_accuracies = []\n",
    "eval_accuracies = []\n",
    "all_labels_roc = []\n",
    "all_preds_roc = [] \n",
    "\n",
    "# Training Loop\n",
    "epochs = 100\n",
    "best_eval_loss = float('inf')  # saving the best model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for _, input_ids, attention_mask, labels in train_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(G, input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Accuracy tracking\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    correct_eval = 0\n",
    "    total_eval = 0\n",
    "    all_labels_roc_epoch = []\n",
    "    all_preds_roc_epoch = []\n",
    "    all_preds = []\n",
    "    all_labels = [] \n",
    "    with torch.no_grad():\n",
    "        for _, input_ids, attention_mask, labels in test_dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(G, input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_eval += (preds == labels).sum().item()\n",
    "            total_eval += labels.size(0)\n",
    "\n",
    "            all_labels_roc_epoch.extend(labels.cpu().numpy())\n",
    "            all_preds_roc_epoch.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_labels_roc = all_labels_roc_epoch\n",
    "    all_preds_roc = all_preds_roc_epoch\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
    "    eval_accuracy = correct_eval / total_eval\n",
    "\n",
    "    eval_losses.append(avg_eval_loss)\n",
    "    eval_accuracies.append(eval_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Evaluation Loss: {avg_eval_loss:.4f}, Evaluation Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "    # I want to save the best model for further training or pack it for inference\n",
    "    if avg_eval_loss < best_eval_loss:\n",
    "        best_eval_loss = avg_eval_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"Best model saved.\")\n",
    "\n",
    "predicted_classes = [1 if prob >= 0.5 else 0 for prob in all_preds_roc]\n",
    "precision = precision_score(all_labels_roc, predicted_classes, average='weighted')\n",
    "recall = recall_score(all_labels_roc, predicted_classes, average='weighted')\n",
    "f1 = f1_score(all_labels_roc, predicted_classes, average='weighted')\n",
    "roc_auc = roc_auc_score(all_labels_roc, all_preds_roc)\n",
    "\n",
    "print(f\"Final Evaluation Results - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, AUC-ROC: {roc_auc:.4f}\")\n",
    "\n",
    "#----------------------------------------------\n",
    "\n",
    "#---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# Visualization: Loss and Accuracy\n",
    "# ============================\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label=\"Training Loss\", color='blue')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.title(\"Training Loss Over Epochs\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Plot Evaluation Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(eval_losses) + 1), eval_losses, marker='x', linestyle='--', label=\"Evaluation Loss\", color='red')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.title(\"Evaluation Loss Over Epochs\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Plot Training Accuracy\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, marker='o', label=\"Training Accuracy\", color='green')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.title(\"Training Accuracy Over Epochs\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Plot Evaluation Accuracy\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(eval_accuracies) + 1), eval_accuracies, marker='x', linestyle='--', label=\"Evaluation Accuracy\", color='purple')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.title(\"Evaluation Accuracy Over Epochs\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Confusion Matrix\n",
    "# ============================\n",
    "\n",
    "# Confusion Matrix with Predicted Classes\n",
    "cm = confusion_matrix(all_labels_roc, predicted_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.title(\"Confusion Matrix (Predicted Classes)\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix with Binary Predictions\n",
    "cm2 = confusion_matrix(all_labels, predicted_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm2, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.title(\"Confusion Matrix (Binary Predictions)\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# AUC-ROC Curve\n",
    "# ============================\n",
    "\n",
    "# AUC-ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(all_labels_roc, all_preds_roc)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC-ROC = {roc_auc:.4f}\", color='tab:blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.title(\"ROC Curve\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Metrics Visualization\n",
    "# ============================\n",
    "\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]\n",
    "scores = [eval_accuracies[-1], precision, recall, f1, roc_auc]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=metrics, y=scores, palette=\"viridis\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.title(\"Evaluation Metrics\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9964f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Calling saved model for further train\n",
    "#----------------------------------\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load the saved model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.to(device)\n",
    "print(\"Loaded the best model.\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "new_train_losses = []\n",
    "new_eval_losses = []\n",
    "new_train_accuracies = []\n",
    "new_eval_accuracies = []\n",
    "\n",
    "# Training for additional epochs\n",
    "additional_epochs = 40\n",
    "best_new_eval_loss = float('inf')\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for _, input_ids, attention_mask, labels in train_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(G, input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    new_train_losses.append(avg_train_loss)\n",
    "    new_train_accuracies.append(train_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    correct_eval = 0\n",
    "    total_eval = 0\n",
    "    all_labels_roc_epoch = []\n",
    "    all_preds_roc_epoch = []\n",
    "    with torch.no_grad():\n",
    "        for _, input_ids, attention_mask, labels in test_dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(G, input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_eval += (preds == labels).sum().item()\n",
    "            total_eval += labels.size(0)\n",
    "\n",
    "            all_labels_roc_epoch.extend(labels.cpu().numpy())\n",
    "            all_preds_roc_epoch.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
    "    eval_accuracy = correct_eval / total_eval\n",
    "\n",
    "    new_eval_losses.append(avg_eval_loss)\n",
    "    new_eval_accuracies.append(eval_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Evaluation Loss: {avg_eval_loss:.4f}, Evaluation Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "    if avg_eval_loss < best_new_eval_loss:\n",
    "        best_new_eval_loss = avg_eval_loss\n",
    "        torch.save(model.state_dict(), \"best_model_updated.pth\")\n",
    "        print(\"New best model saved.\")\n",
    "\n",
    "predicted_classes = [1 if prob >= 0.5 else 0 for prob in all_preds_roc_epoch]\n",
    "precision = precision_score(all_labels_roc_epoch, predicted_classes, average='weighted')\n",
    "recall = recall_score(all_labels_roc_epoch, predicted_classes, average='weighted')\n",
    "f1 = f1_score(all_labels_roc_epoch, predicted_classes, average='weighted')\n",
    "roc_auc = roc_auc_score(all_labels_roc_epoch, all_preds_roc_epoch)\n",
    "\n",
    "print(f\"Final Evaluation Results After Additional Training - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, AUC-ROC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50348a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Second Visualization\n",
    "# ============================\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label=\"Training Loss\", color='blue')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.title(\"Training Loss Over Epochs\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Plot Evaluation Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(eval_losses) + 1), eval_losses, marker='x', linestyle='--', label=\"Evaluation Loss\", color='red')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.title(\"Evaluation Loss Over Epochs\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Plot Training Accuracy\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, marker='o', label=\"Training Accuracy\", color='green')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.title(\"Training Accuracy Over Epochs\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Plot Evaluation Accuracy\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(eval_accuracies) + 1), eval_accuracies, marker='x', linestyle='--', label=\"Evaluation Accuracy\", color='purple')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.title(\"Evaluation Accuracy Over Epochs\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Confusion Matrix\n",
    "# ============================\n",
    "\n",
    "# Confusion Matrix with Predicted Classes\n",
    "cm = confusion_matrix(all_labels_roc, predicted_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.title(\"Confusion Matrix (Predicted Classes)\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix with Binary Predictions\n",
    "cm2 = confusion_matrix(all_labels, predicted_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm2, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.title(\"Confusion Matrix (Binary Predictions)\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# AUC-ROC Curve\n",
    "# ============================\n",
    "\n",
    "# AUC-ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(all_labels_roc, all_preds_roc)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC-ROC = {roc_auc:.4f}\", color='tab:blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.title(\"ROC Curve\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Metrics Visualization\n",
    "# ============================\n",
    "\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]\n",
    "scores = [eval_accuracies[-1], precision, recall, f1, roc_auc]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=metrics, y=scores, palette=\"viridis\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.title(\"Evaluation Metrics\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829651ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
